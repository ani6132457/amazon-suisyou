# app.py
import os
import re
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from selenium import webdriver
from selenium.webdriver.chrome.options import Options

st.set_page_config(page_title="æ¥½å¤©ç”»åƒ + ç™ºæ³¨æ¨å¥¨", layout="wide")

RAKUTEN_ITEM = "https://item.rakuten.co.jp/hype/{}/"
CACHE_FILE = "image_cache.csv"

# ----- è¶…ã‚³ãƒ³ãƒ‘ã‚¯ãƒˆCSS -----
st.markdown("""
<style>
.block-container {padding-top: 0.4rem; padding-bottom: 0.4rem;}
div[data-testid="stVerticalBlock"] {gap: 0.15rem;}
div[data-testid="stMarkdown"] p {margin:0;}
hr {margin:0.25rem 0;}
</style>
""", unsafe_allow_html=True)

# ---------------- CSV ----------------
def read_inventory_csv(uploaded_file):
    try:
        return pd.read_csv(uploaded_file, encoding="cp932")
    except:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, encoding="utf-8")

def normalize(x):
    if pd.isna(x):
        return ""
    return str(x).strip()

def extract_7digits(sku):
    if not sku:
        return None
    sku = str(sku).strip()
    head = sku.split("X")[0]
    m = re.search(r"(\d{7})", head)
    return m.group(1) if m else None

def extract_color(name):
    if not name:
        return ""
    m = re.search(r"[ï¼ˆ(](.*?)[ï¼‰)]", name)
    return m.group(1) if m else ""

# ---------------- æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ ----------------
def load_cache():
    if os.path.exists(CACHE_FILE):
        return pd.read_csv(CACHE_FILE)
    return pd.DataFrame(columns=["rakuten_url", "image_url"])

def save_cache(df):
    df.to_csv(CACHE_FILE, index=False)

# ---------------- Selenium ----------------
@st.cache_resource
def get_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--window-size=1200,900")
    opts.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)"
    )
    return webdriver.Chrome(options=opts)

def extract_img(html, base_url):
    soup = BeautifulSoup(html, "lxml")
    span = soup.find("span", class_="sale_desc")
    if not span:
        return None
    img = span.find("img")
    if not img:
        return None
    return urljoin(base_url, img.get("src"))

def fetch_image(url):
    driver = get_driver()
    try:
        driver.get(url)
        return extract_img(driver.page_source, driver.current_url)
    except:
        return None

# ---------------- UI ----------------
st.title("ğŸ“¦ ç™ºæ³¨æ¨å¥¨é †")

uploaded = st.file_uploader("CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
if not uploaded:
    st.stop()

df = read_inventory_csv(uploaded)

# å¿…é ˆåˆ—
df["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"] = pd.to_numeric(
    df["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"], errors="coerce"
).fillna(0).astype(int)

df["ASIN"] = df["ASIN"].map(normalize)

if "Merchant SKU" in df.columns:
    df["Merchant SKU"] = df["Merchant SKU"].map(normalize)

if "å•†å“å" in df.columns:
    df["å•†å“å"] = df["å•†å“å"].map(normalize)
else:
    df["å•†å“å"] = ""

df = df.sort_values("æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡", ascending=False)

# -------- æ¤œç´¢ --------
search = st.text_input("ğŸ” SKU / ASIN / å•†å“å æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´ï¼‰")

if search:
    search = search.lower()
    mask = (
        df["ASIN"].str.lower().str.contains(search, na=False)
        | df["Merchant SKU"].str.lower().str.contains(search, na=False)
        | df["å•†å“å"].str.lower().str.contains(search, na=False)
    )
    df = df[mask]

# æ¥½å¤©URLç”Ÿæˆ
def build_url(row):
    code = extract_7digits(row["Merchant SKU"])
    if code:
        return RAKUTEN_ITEM.format(code)
    return ""

df["rakuten_url"] = df.apply(build_url, axis=1)

# ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
cache_df = load_cache()
cache_dict = dict(zip(cache_df["rakuten_url"], cache_df["image_url"]))

# è¡¨ç¤ºä»¶æ•°
max_rows = st.number_input("è¡¨ç¤ºä»¶æ•°", 50, 2000, 300, 50)
img_size = st.slider("ç”»åƒã‚µã‚¤ã‚º", 25, 70, 35)

rows = df.head(int(max_rows))

driver = get_driver()

for _, row in rows.iterrows():

    sku = row["Merchant SKU"]
    asin = row["ASIN"]
    name = row["å•†å“å"]
    qty = row["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"]
    url = row["rakuten_url"]

    color = extract_color(name)

    col1, col2, col3 = st.columns([0.32, 4, 0.8])

    # ---- ç”»åƒ ----
with col1:
    img_url = cache_dict.get(url)

    if img_url:
        st.markdown(
            f"""
            <div style="
                width:{img_size}px;
                height:{img_size}px;
                display:flex;
                align-items:center;
                justify-content:center;
                overflow:hidden;
                border-radius:4px;
            ">
                <img src="{img_url}"
                     style="
                         max-width:100%;
                         max-height:100%;
                         object-fit:contain;
                     ">
            </div>
            """,
            unsafe_allow_html=True
        )
        else:
            if url:
                new_img = fetch_image(url)
                if new_img:
                    cache_dict[url] = new_img
                    cache_df.loc[len(cache_df)] = [url, new_img]
                    save_cache(cache_df)
                    st.image(new_img, width=img_size)
                else:
                    st.caption("â€”")

    # ---- SKU / ASIN / ã‚«ãƒ©ãƒ¼ ----
    with col2:
        line = f"SKU:{sku} | ASIN:{asin}"
        if color:
            line += f" | <b>{color}</b>"
        st.markdown(line, unsafe_allow_html=True)

    # ---- ç™ºæ³¨æ¨å¥¨ ----
    with col3:
        st.markdown(
            f"""
            <div style="
                padding:4px;
                text-align:center;
                background:rgba(255,0,0,0.12);
                border-radius:6px;">
            <div style="font-size:9px;">ç™ºæ³¨</div>
            <div style="font-size:17px;font-weight:900;color:#d40000;">
            {qty}
            </div>
            </div>
            """,
            unsafe_allow_html=True,
        )

    st.markdown("<hr>", unsafe_allow_html=True)
