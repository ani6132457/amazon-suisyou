# app.py
import os
import re
import pandas as pd
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urljoin

from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By

st.set_page_config(page_title="æ¥½å¤©ç”»åƒ + ç™ºæ³¨æ¨å¥¨ï¼ˆé«˜é€Ÿãƒ»æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰", layout="wide")

RAKUTEN_ITEM = "https://item.rakuten.co.jp/hype/{}/"
CACHE_FILE = "image_cache.csv"

# ---------------- CSV ----------------
def read_inventory_csv(uploaded_file):
    try:
        return pd.read_csv(uploaded_file, encoding="cp932")
    except:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, encoding="utf-8")

def normalize(x):
    if pd.isna(x):
        return ""
    return str(x).strip()

def extract_7digits(sku):
    if not sku:
        return None
    sku = str(sku).strip()
    head = sku.split("X")[0]
    m = re.search(r"(\d{7})", head)
    if m:
        return m.group(1)
    return None

# ---------------- æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ ----------------
def load_cache():
    if os.path.exists(CACHE_FILE):
        return pd.read_csv(CACHE_FILE)
    return pd.DataFrame(columns=["rakuten_url", "image_url"])

def save_cache(df):
    df.to_csv(CACHE_FILE, index=False)

# ---------------- Selenium ----------------
@st.cache_resource
def get_driver():
    opts = Options()
    opts.add_argument("--headless=new")
    opts.add_argument("--no-sandbox")
    opts.add_argument("--disable-dev-shm-usage")
    opts.add_argument("--disable-gpu")
    opts.add_argument("--window-size=1200,900")
    opts.add_argument("--lang=ja-JP")
    opts.add_argument(
        "--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/121.0.0.0 Safari/537.36"
    )

    driver = webdriver.Chrome(options=opts)
    driver.set_page_load_timeout(20)
    return driver

def extract_img(html, base_url):
    soup = BeautifulSoup(html, "lxml")
    span = soup.find("span", class_="sale_desc")
    if not span:
        return None
    img = span.find("img")
    if not img:
        return None
    src = img.get("src")
    if not src:
        return None
    return urljoin(base_url, src)

def fetch_image(url):
    driver = get_driver()
    try:
        driver.get(url)
        html = driver.page_source
        return extract_img(html, driver.current_url)
    except:
        return None

# ---------------- UI ----------------
st.title("ğŸ“¦ ç™ºæ³¨æ¨å¥¨é † + æ¥½å¤©ç”»åƒï¼ˆé«˜é€Ÿè¡¨ç¤º + æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥ï¼‰")

uploaded = st.file_uploader("CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
if not uploaded:
    st.stop()

df = read_inventory_csv(uploaded)

required = ["ASIN", "æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"]
for col in required:
    if col not in df.columns:
        st.error(f"{col} ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        st.stop()

df["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"] = pd.to_numeric(
    df["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"], errors="coerce"
).fillna(0).astype(int)

if "Merchant SKU" in df.columns:
    df["Merchant SKU"] = df["Merchant SKU"].map(normalize)

df = df.sort_values("æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡", ascending=False).reset_index(drop=True)

# æ¥½å¤©URLç”Ÿæˆ
def build_url(row):
    sku = row.get("Merchant SKU", "")
    code = extract_7digits(sku)
    if code:
        return RAKUTEN_ITEM.format(code)
    return ""

df["rakuten_url"] = df.apply(build_url, axis=1)

# æ°¸ç¶šã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿
cache_df = load_cache()
cache_dict = dict(zip(cache_df["rakuten_url"], cache_df["image_url"]))

# è¡¨ç¤ºä»¶æ•°
max_rows = st.number_input("è¡¨ç¤ºä»¶æ•°", 50, 2000, 200, 50)
img_size = st.slider("ç”»åƒã‚µã‚¤ã‚º", 30, 120, 45)

rows = df.head(int(max_rows))

# ---------------- å…ˆã«æ–‡å­—ã‚’å³è¡¨ç¤º ----------------
containers = []
for idx, row in rows.iterrows():
    c = st.container()
    containers.append((c, row))

# ---------------- ç”»åƒå–å¾—ï¼ˆè‡ªå‹•å®Ÿè¡Œï¼‰ ----------------
driver = get_driver()

for container, row in containers:
    with container:
        sku = row.get("Merchant SKU", "")
        asin = row.get("ASIN", "")
        qty = row.get("æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡", 0)
        url = row.get("rakuten_url", "")

        col1, col2, col3 = st.columns([0.4, 3.5, 1])

        # ---- ç”»åƒ ----
        with col1:
            img_url = cache_dict.get(url)

            if img_url:
                st.image(img_url, width=img_size)
            else:
                st.caption("å–å¾—ä¸­...")
                if url:
                    new_img = fetch_image(url)
                    if new_img:
                        cache_dict[url] = new_img
                        cache_df.loc[len(cache_df)] = [url, new_img]
                        save_cache(cache_df)
                        st.image(new_img, width=img_size)
                    else:
                        st.caption("ãªã—")

        # ---- æƒ…å ±ï¼ˆå³è¡¨ç¤ºï¼‰----
        with col2:
            st.markdown(f"**{sku}**  |  ASIN: {asin}")

        # ---- ç™ºæ³¨æ¨å¥¨ ----
        with col3:
            st.markdown(
                f"""
                <div style="
                    border-radius:8px;
                    padding:6px;
                    text-align:center;
                    background:rgba(255,0,0,0.08);
                ">
                <div style="font-size:11px;">ç™ºæ³¨æ¨å¥¨</div>
                <div style="font-size:20px;font-weight:900;color:#d40000;">
                {qty}
                </div>
                </div>
                """,
                unsafe_allow_html=True,
            )

        st.divider()
