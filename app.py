# app.py
import re
import pandas as pd
import requests
import streamlit as st
from bs4 import BeautifulSoup
from urllib.parse import urljoin

st.set_page_config(page_title="æ¥½å¤©ç”»åƒ + ç™ºæ³¨æ¨å¥¨ï¼ˆsale_descæ–¹å¼ï¼‰", layout="wide")

# æ¥½å¤©URLï¼ˆã‚ãªãŸæŒ‡å®šï¼‰
RAKUTEN_ITEM = "https://item.rakuten.co.jp/hype/{}/"

# å¼¾ã‹ã‚Œã«ãã„ãƒ˜ãƒƒãƒ€
DEFAULT_HEADERS = {
    "User-Agent": (
        "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/121.0.0.0 Safari/537.36"
    ),
    "Accept-Language": "ja-JP,ja;q=0.9,en-US;q=0.8,en;q=0.7",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Connection": "keep-alive",
    "Referer": "https://www.rakuten.co.jp/",
}

# ----------------- CSV -----------------
def read_inventory_csv(uploaded_file) -> pd.DataFrame:
    try:
        return pd.read_csv(uploaded_file, encoding="cp932")
    except Exception:
        uploaded_file.seek(0)
        return pd.read_csv(uploaded_file, encoding="utf-8")

def normalize_text(x) -> str:
    if pd.isna(x):
        return ""
    return str(x).strip()

# ----------------- SKU -> 7æ¡ -----------------
def extract_7digits_from_sku(sku: str) -> str | None:
    """
    SKUã® X ã‚ˆã‚Šå‰ã®éƒ¨åˆ†ã‹ã‚‰7æ¡ã®æ•°å­—ã‚’å–ã‚Šå‡ºã™æƒ³å®šã€‚
    ä¾‹: 7987482X11Y11 -> 7987482
    """
    if not sku:
        return None
    sku = str(sku).strip()

    head = sku.split("X")[0]  # Xã‚ˆã‚Šå‰
    m = re.search(r"(\d{7})", head)
    if m:
        return m.group(1)

    # å¿µã®ãŸã‚å…¨ä½“ã‹ã‚‰ã‚‚æ‹¾ã†
    m2 = re.search(r"(\d{7})", sku)
    return m2.group(1) if m2 else None

# ----------------- VBAäº’æ›ï¼šsale_descå†…img src æŠ½å‡ºï¼ˆbytesè§£æï¼‰ -----------------
def extract_img_from_sale_desc_bytes(html_bytes: bytes, base_url: str) -> str | None:
    """
    <span class="sale_desc"> ã®ä¸­ã®æœ€åˆã® <img> ã‚’æ‹¾ã£ã¦ src ã‚’è¿”ã™
    """
    soup = BeautifulSoup(html_bytes, "lxml")  # bytesã®ã¾ã¾è§£æï¼ˆå®‰å®šï¼‰

    span = soup.find("span", class_="sale_desc")
    if not span:
        return None

    img = span.find("img")
    if not img:
        return None

    src = (img.get("src") or "").strip()
    if not src:
        return None

    return urljoin(base_url, src)

@st.cache_data(show_spinner=False, ttl=60 * 60 * 24)
def fetch_page_and_extract_image(url: str) -> dict:
    """
    URLã‚’å–å¾—ã—ã¦ç”»åƒURLã‚’è¿”ã™ï¼ˆãƒ‡ãƒãƒƒã‚°æƒ…å ±ã‚‚è¿”ã™ï¼‰
    return: {
      "img_url": str|None,
      "status": str,
      "title": str,
      "head": str(å…ˆé ­HTML),
      "final_url": str
    }
    """
    if not url:
        return {"img_url": None, "status": "URLãªã—", "title": "", "head": "", "final_url": ""}

    try:
        with requests.Session() as s:
            r = s.get(url, headers=DEFAULT_HEADERS, timeout=20, allow_redirects=True)

        final_url = r.url
        if r.status_code != 200:
            return {
                "img_url": None,
                "status": f"HTTP {r.status_code}",
                "title": "",
                "head": (r.content[:1500].decode("utf-8", errors="ignore")),
                "final_url": final_url,
            }

        img_url = extract_img_from_sale_desc_bytes(r.content, base_url=final_url)
        soup = BeautifulSoup(r.content, "lxml")
        title = soup.title.get_text(strip=True) if soup.title else ""

        head = r.content[:2000].decode("utf-8", errors="ignore")

        if img_url:
            return {"img_url": img_url, "status": "OK", "title": title, "head": head, "final_url": final_url}
        else:
            return {
                "img_url": None,
                "status": "sale_descãªã—/ imgãªã—",
                "title": title,
                "head": head,
                "final_url": final_url,
            }

    except Exception as e:
        return {"img_url": None, "status": f"ERROR: {type(e).__name__}", "title": "", "head": "", "final_url": ""}

def choose_page_url(row: pd.Series, url_colname: str | None) -> str | None:
    """
    å–å¾—å…ƒURLã‚’æ±ºã‚ã‚‹ï¼š
    1) CSVå†…ã«URLåˆ—ãŒã‚ã‚‹ãªã‚‰ãã‚Œã‚’å„ªå…ˆï¼ˆVBAã¨åŒã˜é‹ç”¨ãŒå¯èƒ½ï¼‰
    2) ãªã‘ã‚Œã° SKU ã‹ã‚‰7æ¡ã‚’æŠœã„ã¦ æ¥½å¤©URLç”Ÿæˆ
    """
    if url_colname and url_colname in row.index:
        u = normalize_text(row.get(url_colname, ""))
        if u.startswith("http"):
            return u

    sku = normalize_text(row.get("Merchant SKU", ""))
    code7 = extract_7digits_from_sku(sku)
    if code7:
        return RAKUTEN_ITEM.format(code7)

    return None

# ----------------- UI -----------------
st.title("ğŸ“¦ ç™ºæ³¨æ¨å¥¨é † + æ¥½å¤©ç”»åƒå–å¾—ï¼ˆVBAäº’æ›: sale_descâ†’img srcï¼‰")
st.caption("æ¥½å¤©ãƒšãƒ¼ã‚¸ã®ã‚½ãƒ¼ã‚¹ã«ã‚ã‚‹ `<span class='sale_desc'>` å†…ã® `<img src>` ã‚’å–å¾—ã—ã¦è¡¨ç¤ºã—ã¾ã™ã€‚")

uploaded = st.file_uploader("CSVã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰", type=["csv"])
if not uploaded:
    st.stop()

df = read_inventory_csv(uploaded)

# å¿…é ˆåˆ—
required_cols = ["ASIN", "æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"]
missing = [c for c in required_cols if c not in df.columns]
if missing:
    st.error(f"CSVã«å¿…è¦ãªåˆ—ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {missing}")
    st.stop()

# æ•°å€¤åŒ–
df["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"] = pd.to_numeric(df["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"], errors="coerce").fillna(0).astype(int)

# æ–‡å­—åˆ—åŒ–
df["ASIN"] = df["ASIN"].map(normalize_text)
if "Merchant SKU" in df.columns:
    df["Merchant SKU"] = df["Merchant SKU"].map(normalize_text)

# URLåˆ—å€™è£œï¼ˆã‚ã‚Œã°é¸æŠã§ãã‚‹ï¼‰
url_candidates = [c for c in df.columns if "url" in c.lower() or "URL" in c or "Url" in c]
url_colname = None
if url_candidates:
    url_colname = st.selectbox(
        "ï¼ˆä»»æ„ï¼‰å–å¾—å…ƒURLã®åˆ—ã‚’é¸æŠï¼ˆVBAã®Cåˆ—ã«ç›¸å½“ã€‚ç„¡ã‘ã‚Œã°SKUâ†’7æ¡ã§ç”Ÿæˆï¼‰",
        ["(ä½¿ã‚ãªã„)"] + url_candidates,
        index=0,
    )
    if url_colname == "(ä½¿ã‚ãªã„)":
        url_colname = None
else:
    st.info("CSVå†…ã«URLã£ã½ã„åˆ—ãŒç„¡ã„ã®ã§ã€SKUâ†’7æ¡â†’æ¥½å¤©URLã§å–å¾—ã—ã¾ã™ã€‚")

# ãƒ•ã‚£ãƒ«ã‚¿UI
left, mid, right = st.columns([1.6, 1.1, 1.3], gap="large")
with left:
    query = st.text_input("ğŸ” SKU ã¾ãŸã¯ ASIN ã§æ¤œç´¢ï¼ˆéƒ¨åˆ†ä¸€è‡´OKï¼‰", placeholder="ä¾‹: 7987070 / B0DG... / 7987 ...")
    st.caption("ã‚¹ãƒšãƒ¼ã‚¹åŒºåˆ‡ã‚Šã§è¤‡æ•°æŒ‡å®šã™ã‚‹ã¨ AND æ¤œç´¢ã«ãªã‚Šã¾ã™ã€‚")
with mid:
    only_positive = st.checkbox("ç™ºæ³¨æ¨å¥¨ãŒ0ã¯é™¤å¤–", value=True)
    min_qty = st.number_input("æœ€ä½ç™ºæ³¨æ¨å¥¨æ•°", min_value=0, value=1, step=1)
with right:
    max_cards = st.number_input("æœ€å¤§è¡¨ç¤ºä»¶æ•°", min_value=1, max_value=5000, value=200, step=50)
    img_width = st.slider("ç”»åƒã‚µã‚¤ã‚º", min_value=30, max_value=200, value=60, step=10)

debug = st.checkbox("ãƒ‡ãƒãƒƒã‚°è¡¨ç¤ºï¼ˆç”»åƒãŒå‡ºãªã„åŸå› ç¢ºèªï¼‰", value=False)

view = df.copy()
if only_positive:
    view = view[view["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"] > 0]
view = view[view["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"] >= int(min_qty)]

# æ¤œç´¢ï¼ˆSKU/ASINï¼‰
q = (query or "").strip()
if q:
    tokens = [t for t in re.split(r"\s+", q) if t]
    if "Merchant SKU" in view.columns:
        hay = (view["Merchant SKU"].fillna("") + " " + view["ASIN"].fillna("")).str.lower()
    else:
        hay = view["ASIN"].fillna("").str.lower()

    mask = pd.Series(True, index=view.index)
    for t in tokens:
        t = t.lower()
        mask &= hay.str.contains(re.escape(t), na=False)
    view = view[mask]

# ä¸¦ã¹æ›¿ãˆï¼ˆå¤šã„é †ï¼‰
view = view.sort_values("æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡", ascending=False).reset_index(drop=True)

st.write(f"è¡¨ç¤ºä»¶æ•°: **{len(view)}**")
st.divider()

# è»½é‡ãƒ†ãƒ¼ãƒ–ãƒ«
with st.expander("ä¸€è¦§ãƒ†ãƒ¼ãƒ–ãƒ«ï¼ˆè»½é‡ï¼‰"):
    show_cols = [c for c in ["Merchant SKU", "å•†å“å", "ASIN", "æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"] if c in view.columns]
    st.dataframe(view[show_cols], use_container_width=True, height=320)

# ã‚«ãƒ¼ãƒ‰è¡¨ç¤º
view_cards = view.head(int(max_cards))

for _, row in view_cards.iterrows():
    asin = normalize_text(row["ASIN"])
    qty = int(row["æ¨å¥¨ã•ã‚Œã‚‹åœ¨åº«è£œå……æ•°é‡"])
    sku = normalize_text(row.get("Merchant SKU", ""))
    name = normalize_text(row.get("å•†å“å", ""))

    page_url = choose_page_url(row, url_colname=url_colname)
    result = fetch_page_and_extract_image(page_url) if page_url else {"img_url": None, "status": "URLç”Ÿæˆä¸å¯", "title": "", "head": "", "final_url": ""}

    img_url = result["img_url"]
    status = result["status"]
    title = result["title"]
    head = result["head"]
    final_url = result["final_url"]

    # å•†å“ã‚«ãƒ¼ãƒ‰æ 
    st.markdown(
        """
        <div style="
            border: 1px solid rgba(0,0,0,0.08);
            border-radius: 14px;
            padding: 12px 12px 6px 12px;
            background: rgba(0,0,0,0.015);
        ">
        """,
        unsafe_allow_html=True,
    )

    col_img, col_info, col_qty = st.columns([0.6, 3.8, 1.2], gap="medium")

    with col_img:
        if img_url:
            st.image(img_url, width=int(img_width))
        else:
            st.caption(f"ç”»åƒãªã—\n({status})")

    with col_info:
        if sku:
            st.markdown(f"**SKU:** `{sku}`")
        st.markdown(f"**ASIN:** `{asin}`")
        if name:
            st.caption(name)

        if page_url:
            st.markdown(f"**å–å¾—å…ƒURL:** {page_url}")
        if final_url and final_url != page_url:
            st.caption(f"ãƒªãƒ€ã‚¤ãƒ¬ã‚¯ãƒˆå…ˆ: {final_url}")
        if debug:
            st.caption(f"status: {status} / title: {title}")
            st.code(head, language="html")

    with col_qty:
        st.markdown(
            f"""
            <div style="
                border-radius: 14px;
                padding: 12px 10px;
                border: 1px solid rgba(255,0,0,0.25);
                background: rgba(255,0,0,0.07);
                text-align: center;
            ">
                <div style="font-size: 12px; opacity: 0.75;">ç™ºæ³¨æ¨å¥¨</div>
                <div style="font-size: 36px; font-weight: 900; color: #d40000; line-height: 1.05;">
                    {qty}
                </div>
            </div>
            """,
            unsafe_allow_html=True,
        )

    st.markdown("</div>", unsafe_allow_html=True)
    st.divider()
